<!DOCTYPE html>
<html lang="de">
<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MTVVNDHYS8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MTVVNDHYS8');
</script>

    <meta charset="utf-8">
    <title>Timo Freiesleben Website</title>
    <link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <link href="styles/style.css" rel="stylesheet" type="text/css">
		
<body><div align="center">
<h1>Timo Freiesleben</h1>
	<nav><br>
					<button type="button" id="start" onclick="window.location.href='index.html';">About</button>
					<button type="button" id="research" autofocus onclick="window.location.href='research.html';">Research</button>
					<button type="button" id="teaching" onclick="window.location.href='teaching.html';">Teaching</button>
					<button type="button" id="contact" onclick="window.location.href='contact.html';">Contact</button>
          </nav>
 </div>
  	<main id="main">
	<h2>
	While practical applications of machine learning are ubiquitous, the theoretical and philosophical foundations of machine learning are lagging behind.
	Core concepts like interpretability, generalization, robustness, uncertainty, overfitting, benchmarks, etc. are often only vaguely defined.
	However, without precise definitions and conceptual foundations, machine learning will remain engineering and not become a real science.
	Worse still, it will not become a reliable tool for either scientific or industrial applications.
	<br><br>
	The goal of my research is to address these fundamental challenges.
	 My approach combines philosophical conceptual analysis with rigourous mathematical modeling. 
	 I consider myself a mediator between disciplines: I am a philosopher close to machine learning practice, and a philosophically inclined AI researcher;
	 but faculty lines are so 20th century anyway.	You can find all my research on
	my <a href='https://scholar.google.com/citations?user=BUvMXaAAAAAJ&hl=en'  target="_blank">Google Scholar profile</a>. 
	 </br></br>
	 I have worked on the following research projects:
	 	<ul>
		<li> <a href='https://ml-science-book.com/' target="_blank"><b>Supervised Machine Learning for Science:</b></a> In this book, 
		<a href='https://christophmolnar.com/' target="_blank">Christoph Molnar</a> and I explore how machine learning is changing science, what kinds of scientific questions it does (not) allow to address, and how we can integrate 
		domain knowledge, uncertainty quantification, and interpretability to make machine learning a proper scientific tool.  
		<br>
		<img src="images/book.jpg" id='scholarIm' width=50% border="0" onclick="window.open('https://ml-science-book.com/','_blank');">
		<br>
		</li>
		<li> <a href='https://arxiv.org/abs/2306.04292' target="_blank"><b>Dear XAI Community, We Need to Talk! :</b></a> In this paper,
		<a href='https://gunnarkoenig.com/' target="_blank">Gunnar König</a> and I discuss fundamental misconceptions in current XAI research. For example, many XAI techniques are proposed without a clear purpose.
		We think that if these misconceptions are not resolved, XAI will not become a mainstay in machine learning.	
				<br>
		<img src="images/dear.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/chapter/10.1007/978-3-031-44064-9_3','_blank');">
		<br>
		</li>
		<li> <a href='https://link.springer.com/article/10.1007/s11023-024-09691-z?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=oa_20240715&utm_content=10.1007%2Fs11023-024-09691-z' target="_blank"><b>
		Scientific Inference with Interpretable Machine Learning :</b></a> In this paper, 
		<a href='https://gunnarkoenig.com/' target="_blank">Gunnar König</a>, <a href='https://christophmolnar.com/' target="_blank">Christoph Molnar</a>
, <a href='https://mlcolab.org/resources/team/lvaro-tejero-cantero' target="_blank">Alvaro Tejero-Cantero</a> and I discuss how interpretable machine learning can be turned into a tool for scientific inference. 
We show that properties that were represented by interpretable parameters in traditional models, 
can be represented in machine learning models by summary statistics of model behaviour, we call them property descriptions.
		<br>
		<img src="images/inference.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/article/10.1007/s11023-024-09691-z?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=oa_20240715&utm_content=10.1007%2Fs11023-024-09691-z','_blank');">
		<br>
</li>
		<li> <a href='https://link.springer.com/chapter/10.1007/978-3-031-44064-9_24' target="_blank"><b>
		Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process :</b></a> In this paper, 
		<a href='https://gunnarkoenig.com/' target="_blank">Gunnar König</a>, <a href='https://christophmolnar.com/' target="_blank">Christoph Molnar</a>, others, and I apply partial dependence plots and permutation feature importance directly
		to the data-generating mechanism. We analyze and quantify the deviations between these ground-truth values and those obtained by applying the same methods to a trained machine learning model.
		<img src="images/relating.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/chapter/10.1007/978-3-031-44064-9_24','_blank');">
		<br>
</li>
		<li> <a href='https://link.springer.com/article/10.1007/s11229-023-04334-9' target="_blank"><b>
		A Theory of Robustness in Machine Learning :</b></a> In this paper, 
		<a href='https://sites.google.com/view/thomas-grote/startseite' target="_blank">Thomas Grote</a> and I show how different conceptions of robustness in machine learning can be unified. We describe robustess as the
relative stability of a robustness target with respect to specific interventions on a modifier.	
		<br>
		<img src="images/robustness.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/article/10.1007/s11229-023-04334-9','_blank');">
		<br>
</li>
		<li> <a href='https://ojs.aaai.org/index.php/AAAI/article/view/26398' target="_blank"><b>
		Improvement-focused causal recourse :</b></a> In this paper, 
		<a href='https://gunnarkoenig.com/' target="_blank">Gunnar König</a>, <a href='https://scholar.google.com/citations?user=15GNzKcAAAAJ&hl=de' target="_blank">Moritz Grosse-Wentrup</a>  and 
I argue that the
current implementations of counterfactual explanations show users how they can trick the machine learning system. We show how causal knowledge helps to avoid this by 
giving users meaningful recommendations for action that improve their qualification.	
 		<br>
		<img src="images/improvement.png" id='scholarIm' width=50% border="0" onclick="window.open('https://ojs.aaai.org/index.php/AAAI/article/view/26398','_blank');">
		<br>
 </li>
  		<li> <a href='https://arxiv.org/abs/2312.05337' target="_blank"><b>
		Artificial Neural Nets and the Representation of Human Concepts :</b></a> In this paper, I explore the concepts learned by neural network models trained through supervised learning. 
		I challenge the widely held assumption that these networks encode known human concepts in individual units.
 		<br>
		<img src="images/act.png" id='scholarIm' width=50% border="0" onclick="window.open('https://arxiv.org/abs/2312.05337','_blank');">
		<br>
 </li>
 <li> <a href='https://www.nature.com/articles/s42256-024-00924-5' target="_blank"><b>
		Foundation models in healthcare require rethinking reliability :</b></a> In this short commentary, 
		<a href='https://sites.google.com/view/thomas-grote/startseite' target="_blank">Thomas Grote</a>, <a href='https://hertie.ai/data-science/team/members/philipp-berens' target="_blank">Philipp Berens</a>,
		and I explore why the standard practice of assessing machine learning model reliability through out-of-sample testing is insufficient for foundation models. 
		We advocate for the development of a new framework for reliability testing tailored specifically to foundation models.		
		<br>
		<img src="images/foundation.jpg" id='scholarIm' width=50% border="0" onclick="window.open('https://www.nature.com/articles/s42256-024-00924-5','_blank');">
		<br>
</li>
 		<li> <a href='https://link.springer.com/article/10.1007/s11023-021-09580-9' target="_blank"><b>
		The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples :</b></a> In this paper, I examine the relationship between counterfactual explanations and adversarial examples, 
		both of which can be derived by solving the same optimization problem. I argue that the key conceptual distinction lies in the fact that adversarial examples must necessarily be misclassified. 
		Furthermore, I emphasize that counterfactual explanations achieve their full explanatory potential only when they are positioned as close as possible to the original data point.
 		<br>
		<img src="images/adversarial.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/article/10.1007/s11023-021-09580-9','_blank');">
		<br>
 </li>
 		<li> <a href='https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4' target="_blank"><b>
		General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models :</b></a> This was a really cool group project initiated by <a href='https://christophmolnar.com/' target="_blank">Christoph Molnar</a>,
		in which my colleagues and I explored common mistakes that can occur when interpreting machine learning models using model-agnostic interpretation techniques (see table below). In my view, the most significant pitfall is ignoring feature dependencies.
		<br>
		<img src="images/pitfalls.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4','_blank');">
		<br>
</li>
		<li> <a href='https://link.springer.com/chapter/10.1007/978-3-031-63800-8_5' target="_blank"><b>
		CountARFactuals – Generating Plausible Model-Agnostic Counterfactual Explanations with Adversarial Random Forests :</b></a> In this paper, 
		<a href='https://scholar.google.de/citations?user=pwoXLlsAAAAJ&hl=de' target="_blank">Susanne Dandl</a>, <a href='https://scholar.google.com.au/citations?user=WXOQwicAAAAJ&hl=de' target="_blank">Kristin Blesch</a>,
		<a href='https://gunnarkoenig.com/' target="_blank">Gunnar König</a>,
		<a href='https://scholar.google.com.au/citations?user=4_VM5MIAAAAJ&hl=de' target="_blank">Marvin Wright</a>, others, and I  introduce an efficient algorithm to generate plausible counterfactual explanations.
 		<br>
		<img src="images/arf.png" id='scholarIm' width=50% border="0" onclick="window.open('https://link.springer.com/chapter/10.1007/978-3-031-63800-8_5','_blank');">
		<br>
 </li>

	</ul>
	</br></br>

	
	</h2>
		<br>
		<h3>
							
				<div align="center">Find me on 
				<a href='https://scholar.google.com/citations?user=BUvMXaAAAAAJ&hl=en' target="_blank">
				google scholar
				</a>
				<img src="images/scholar.png" id='scholarIm' width=6% border="0" onclick="window.open('https://scholar.google.com/citations?user=BUvMXaAAAAAJ&hl=en','_blank');">
				</div>
		</h3>

  </main>
  	<footer>
<br><br>
	<b>Impressum: </b>Timo Freiesleben, Maria-von-Linden-Straße 6, 72076 T&uuml;bingen.
	</footer>
</body>
</html>